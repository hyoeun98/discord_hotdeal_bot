{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver import Keys, ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "from collections import deque\n",
    "import json\n",
    "\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "ARCA_LIVE_LINK = \"https://arca.live/b/hotdeal\"\n",
    "RULI_WEB_LINK = \"https://bbs.ruliweb.com/market/board/1020?view=default\"\n",
    "PPOM_PPU_LINK = \"https://www.ppomppu.co.kr/zboard/zboard.php?id=ppomppu\"\n",
    "QUASAR_ZONE_LINK = \"https://quasarzone.com/bbs/qb_saleinfo\"\n",
    "FM_KOREA_LINK = \"https://www.fmkorea.com/hotdeal\"\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    acks=0, # 메시지 전송 완료에 대한 체크\n",
    "    compression_type='gzip', # 메시지 전달할 때 압축(None, gzip, snappy, lz4 등)\n",
    "    bootstrap_servers=['localhost:29092', 'localhost:39092', 'localhost:49092'], # 전달하고자 하는 카프카 브로커의 주소 리스트\n",
    "    value_serializer=lambda x:json.dumps(x, default=str).encode('utf-8') # 메시지의 값 직렬화\n",
    ")\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'test', # 토픽명\n",
    "    bootstrap_servers=['localhost:29092', 'localhost:39092', 'localhost:49092'], # 카프카 브로커 주소 리스트\n",
    "    auto_offset_reset='earliest', # 오프셋 위치(earliest:가장 처음, latest: 가장 최근)\n",
    "    enable_auto_commit=True, # 오프셋 자동 커밋 여부\n",
    "    group_id='test-group', # 컨슈머 그룹 식별자\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8')), # 메시지의 값 역직렬화\n",
    "    consumer_timeout_ms=10000 # 데이터를 기다리는 최대 시간\n",
    ")\n",
    "\n",
    "class PAGES:\n",
    "    def __init__(self):\n",
    "        self.refresh_delay = 60 # sec\n",
    "        self.item_link_queue = deque()\n",
    "        self.previous_items_queue = deque()\n",
    "        \n",
    "    def set_drvier(self, site_name):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "        chrome_options.add_argument('--block-new-web-contents')\n",
    "        driver = webdriver.Chrome(options = chrome_options)\n",
    "        driver.implicitly_wait(10)\n",
    "        driver.get(site_name)\n",
    "        return driver\n",
    "    \n",
    "    def pub_hot_deal_page(self, page, item_link):\n",
    "        producer.send(topic = 'test', headers = [(\"page\", page.encode(\"utf-8\"))], value=item_link)\n",
    "        producer.flush()\n",
    "\n",
    "    \n",
    "    def error_logging(self, e: Exception, error_type, **kwargs):\n",
    "        error_log = {\"error_log\": e.__str__(), \"time\": time.ctime(), \"error_type\": error_type}\n",
    "        if kwargs:\n",
    "            for k, v in kwargs:\n",
    "                error_log[k] = v\n",
    "        # db.error_log.insert_one(error_log)\n",
    "        print(error_log)\n",
    "        \n",
    "class ARCA_LIVE(PAGES): # shopping_mall_link, shopping_mall, item_name, price, delivery, content, comment\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hot_deal_page = ARCA_LIVE_LINK\n",
    "    \n",
    "    def get_item_links(self):\n",
    "        get_item_driver = self.set_drvier(self.hot_deal_page)\n",
    "        while True:\n",
    "            for i in range(4, 49):\n",
    "                try:\n",
    "                    item = get_item_driver.find_element(By.CSS_SELECTOR, f\"body > div.root-container > div.content-wrapper.clearfix > article > div > div.article-list > div.list-table.hybrid > div:nth-child({i}) > div > div > span.vcol.col-title > a\")\n",
    "                    item_link = item.get_attribute(\"href\")\n",
    "                    print(i, item_link)\n",
    "                except Exception as e:\n",
    "                    self.error_logging(e, f\"fail get item links {self.__class__}\")\n",
    "                    \n",
    "                if item_link not in self.previous_items_queue:\n",
    "                    self.item_link_queue.append((item_link, 0))\n",
    "                    self.previous_items_queue.appendleft(item_link)\n",
    "                    if len(self.previous_items_queue) > 100:\n",
    "                        self.previous_items_queue.pop()\n",
    "                else:\n",
    "                    pass\n",
    "            time.sleep(30)\n",
    "            get_item_driver.refresh()\n",
    "            \n",
    "    def crawling(self):\n",
    "        driver = self.set_drvier(self.hot_deal_page)\n",
    "        while True:\n",
    "            try:\n",
    "                item_link, retry_attempt = self.item_link_queue.popleft()\n",
    "                print(item_link, retry_attempt)\n",
    "            except:\n",
    "                print(\"Empty Queue\")\n",
    "                break\n",
    "            driver.get(item_link)\n",
    "            time.sleep(5)\n",
    "            try: # 신고 처리, 보안 검사 등\n",
    "                table = driver.find_element(By.TAG_NAME, \"table\")\n",
    "                rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                details = [row.text for row in rows]\n",
    "                shopping_mall_link, shopping_mall, item_name, price, delivery = list(map(lambda x: \"\".join(x.split()[1:]), details))\n",
    "                content = driver.find_element(By.CSS_SELECTOR, \"body > div.root-container > div.content-wrapper.clearfix > article > div > div.article-wrapper > div.article-body > div.fr-view.article-content\").text\n",
    "                comment_box = driver.find_element(By.CSS_SELECTOR, \"#comment > div.list-area\")\n",
    "                comment = list(map(lambda x: x.text, comment_box.find_elements(By.CLASS_NAME, \"text\")))\n",
    "            except Exception as e:\n",
    "                if retry_attempt >= 3:\n",
    "                    self.error_logging(e, f\"fail crawling {self.__class__}\", item_link = item_link)\n",
    "                else:\n",
    "                    self.item_link_queue.append((item_link, retry_attempt + 1))\n",
    "                continue\n",
    "            \n",
    "            self.insert_to_db(item_link = item_link, shopping_mall_link = shopping_mall_link, shopping_mall = shopping_mall, price = price, item_name = item_name, delivery = delivery, content = content, comment = comment)\n",
    "\n",
    "# shopping_mall_link가 누락된 채로 게시글이 올라옴\n",
    "class RULI_WEB(PAGES): # shopping_mall_link, item_name, content, comment\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hot_deal_page = RULI_WEB_LINK\n",
    "    \n",
    "    def get_item_links(self):\n",
    "        get_item_driver = self.set_drvier(self.hot_deal_page)\n",
    "        while True:\n",
    "            item_table = get_item_driver.find_elements(By.CSS_SELECTOR, \"#board_list > div > div.board_main.theme_default.theme_white.theme_white > table > tbody > tr\")\n",
    "            for i, item in enumerate(item_table):\n",
    "                try:\n",
    "                    if item.get_attribute(\"class\") == \"table_body blocktarget\":\n",
    "                        item_link = item.find_element(By.CSS_SELECTOR, \"td.subject > div > a.deco\").get_attribute(\"href\")\n",
    "                        print(i, item_link)\n",
    "                    else: # 공지, best 핫딜 등\n",
    "                        continue\n",
    "                    \n",
    "                    # if item_link not in self.previous_items_queue:\n",
    "                    #     self.item_link_queue.append((item_link, 0))\n",
    "                    #     self.previous_items_queue.appendleft(item_link)\n",
    "                    #     if len(self.previous_items_queue) > 100:\n",
    "                    #         self.previous_items_queue.pop()\n",
    "                    # else:\n",
    "                    #     pass\n",
    "                except Exception as e:\n",
    "                    self.error_logging(e, f\"fail get item links {self.__class__}\")\n",
    "                    \n",
    "            time.sleep(30)\n",
    "            get_item_driver.refresh()\n",
    "            \n",
    "        \n",
    "    def crawling(self):\n",
    "        driver = self.set_drvier(self.hot_deal_page)\n",
    "        while True:\n",
    "            try:\n",
    "                item_link, retry_attempt = self.item_link_queue.popleft()\n",
    "                print(item_link, retry_attempt)\n",
    "            except:\n",
    "                print(\"Empty Queue\")\n",
    "                break\n",
    "            driver.get(item_link)\n",
    "            time.sleep(5)\n",
    "            try: # 신고 처리, 보안 검사 등\n",
    "                item_name = driver.find_element(By.CSS_SELECTOR, \"#board_read > div > div.board_main > div.board_main_top > div.user_view > div:nth-child(1) > div > h4 > span > span.subject_inner_text\").text\n",
    "                shopping_mall_link = driver.find_element(By.CSS_SELECTOR, \"#board_read > div > div.board_main > div.board_main_view > div.row.relative > div > div.source_url.box_line_with_shadow > a\").text\n",
    "                content = driver.find_element(By.TAG_NAME, \"article\").text\n",
    "                comment = list(map(lambda x: x.text, driver.find_elements(By.CLASS_NAME, \"comment\")))\n",
    "            except Exception as e:\n",
    "                if retry_attempt >= 3:\n",
    "                    self.error_logging(e, f\"fail crawling {self.__class__}\", item_link = item_link)\n",
    "                else:\n",
    "                    self.item_link_queue.append((item_link, retry_attempt + 1))\n",
    "                continue\n",
    "            \n",
    "            self.insert_to_db(item_link = item_link, shopping_mall_link = shopping_mall_link, item_name = item_name, content = content, comment = comment)\n",
    "        \n",
    "class FM_KOREA(PAGES): # shopping_mall_link, shopping_mall, item_name, price, delivery, content, comment\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hot_deal_page = FM_KOREA_LINK\n",
    "    \n",
    "    def get_item_links(self):\n",
    "        get_item_driver = self.set_drvier(self.hot_deal_page)\n",
    "        while True:\n",
    "            for i in range(1, 21):\n",
    "                try:\n",
    "                    item = get_item_driver.find_element(By.CSS_SELECTOR, f\"#bd_1196365581_0 > div > div.fm_best_widget._bd_pc > ul > li:nth-child({i}) > div > h3 > a\")\n",
    "                    item_link = item.get_attribute(\"href\")\n",
    "                    print(i, item_link)\n",
    "                except Exception as e:\n",
    "                    self.error_logging(e, f\"fail get item links {self.__class__}\")\n",
    "                    \n",
    "                if item_link not in self.previous_items_queue:\n",
    "                    self.item_link_queue.append((item_link, 0))\n",
    "                    self.previous_items_queue.appendleft(item_link)\n",
    "                    if len(self.previous_items_queue) > 100:\n",
    "                        self.previous_items_queue.pop()\n",
    "                else:\n",
    "                    pass\n",
    "            time.sleep(30)\n",
    "            get_item_driver.refresh()            \n",
    "            \n",
    "    def crawling(self):\n",
    "        driver = self.set_drvier(self.hot_deal_page)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                item_link, retry_attempt = self.item_link_queue.popleft()\n",
    "                print(item_link, retry_attempt)\n",
    "            except:\n",
    "                print(\"Empty Queue\")\n",
    "                break\n",
    "            driver.get(item_link)\n",
    "            time.sleep(5)\n",
    "            try: # 신고 처리, 보안 검사 등\n",
    "                details = driver.find_elements(By.CLASS_NAME, \"xe_content\")\n",
    "                shopping_mall_link, shopping_mall, item_name, price, delivery, content, *comment = details\n",
    "                shopping_mall_link, shopping_mall, item_name, price, delivery, content = map(lambda x: x.text, (shopping_mall_link, shopping_mall, item_name, price, delivery, content))\n",
    "                comment = list(map(lambda x: x.text, comment))\n",
    "            except Exception as e:\n",
    "                if retry_attempt >= 3:\n",
    "                    self.error_logging(e, f\"fail crawling {self.__class__}\", item_link = item_link)\n",
    "                else:\n",
    "                    self.item_link_queue.append((item_link, retry_attempt + 1))\n",
    "                continue\n",
    "            \n",
    "            self.insert_to_db(item_link = item_link, shopping_mall_link = shopping_mall_link, shopping_mall = shopping_mall, item_name = item_name, price = price, delivery = delivery, content = content, comment = comment)\n",
    "            \n",
    "            \n",
    "def super_crawling():\n",
    "    while True:\n",
    "        messages = page_consumer.poll(timeout_ms=1000)\n",
    "        \n",
    "        if messages:\n",
    "            for topic_partition, records in messages.items():\n",
    "                for record in records:\n",
    "                    print(record)\n",
    "        else:\n",
    "            print(\"123123\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "class QUASAR_ZONE(PAGES): # shopping_mall_link, shopping_mall, item_name, price, delivery, content, comment\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hot_deal_page = QUASAR_ZONE_LINK\n",
    "        \n",
    "    def get_item_links(self):\n",
    "        print(\"get_item_links\")\n",
    "        get_item_driver = self.set_drvier(self.hot_deal_page)\n",
    "        while True:\n",
    "            for i in range(1, 31):\n",
    "                try:\n",
    "                    item = get_item_driver.find_element(By.CSS_SELECTOR, f\"#frmSearch > div > div.list-board-wrap > div.market-type-list.market-info-type-list.relative > table > tbody > tr:nth-child({i}) > td:nth-child(2) > div > div.market-info-list-cont > p > a\")\n",
    "                    item_link = item.get_attribute(\"href\")\n",
    "                    print(i, item_link)\n",
    "                except Exception as e:\n",
    "                    self.error_logging(e, f\"fail get item links {self.__class__}\")\n",
    "                    \n",
    "                if item_link not in self.previous_items_queue:\n",
    "                    self.pub_hot_deal_page(page = self.hot_deal_page, item_link = item_link)\n",
    "                    self.item_link_queue.append((item_link, 0))\n",
    "                    self.previous_items_queue.appendleft(item_link)\n",
    "                    if len(self.previous_items_queue) > 1000:\n",
    "                        self.previous_items_queue.pop()\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            time.sleep(30)\n",
    "            get_item_driver.refresh()\n",
    "        \n",
    "    def crawling(self):\n",
    "        print(\"crawling\")\n",
    "        driver = self.set_drvier(self.hot_deal_page)\n",
    "        while True:\n",
    "            try:\n",
    "                item_link, retry_attempt = self.item_link_queue.popleft()\n",
    "                print(item_link, retry_attempt)\n",
    "            except:\n",
    "                print(\"Empty Queue\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "            driver.get(item_link)\n",
    "            \n",
    "            try: # 신고 처리, 보안 검사 등\n",
    "                item_name = driver.find_element(By.CSS_SELECTOR, \"#content > div > div.sub-content-wrap > div.left-con-wrap > div.common-view-wrap.market-info-view-wrap > div > dl > dt > div:nth-child(1) > h1\").text.split()[2:]\n",
    "                item_name = \" \".join(item_name)\n",
    "                table = driver.find_element(By.TAG_NAME, \"table\")\n",
    "                rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                content = driver.find_element(By.CSS_SELECTOR, \"#new_contents\").text\n",
    "                comment = list(map(lambda x: x.text, driver.find_elements(By.CSS_SELECTOR, \"#content > div.sub-content-wrap > div.left-con-wrap > div.reply-wrap > div.reply-area > div.reply-list\")))\n",
    "            except Exception as e:\n",
    "                if retry_attempt >= 3:\n",
    "                    self.error_logging(e, f\"fail crawling {self.__class__}\", item_link = item_link)\n",
    "                else:\n",
    "                    self.item_link_queue.append((item_link, retry_attempt + 1))\n",
    "                continue\n",
    "            \n",
    "            details = [row.text for row in rows]\n",
    "            shopping_mall_link, shopping_mall, price, delivery, *_ = list(map(lambda x: \"\".join(x.split()[1:]), details))\n",
    "            self.insert_to_db(item_link = item_link, shopping_mall_link = shopping_mall_link, shopping_mall = shopping_mall, item_name = item_name, price = price, delivery = delivery, content = content, comment = comment)\n",
    "\n",
    "# shopping_mall이 tag되지 않은 채로 올라옴\n",
    "class PPOM_PPU(PAGES):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hot_deal_page = PPOM_PPU_LINK\n",
    "        \n",
    "    def get_item_links(self):\n",
    "        get_item_driver = self.set_drvier(self.hot_deal_page)\n",
    "        while True:\n",
    "            for i in range(9, 34):#revolution_main_table > tbody > tr:nth-child(33)\n",
    "                try:#revolution_main_table > tbody > tr:nth-child(9)\n",
    "                    item = get_item_driver.find_element(By.CSS_SELECTOR, f\"#revolution_main_table > tbody > tr:nth-child({i}) > td.baseList-space.title > div > div > a\")\n",
    "                    item_link = item.get_attribute(\"href\")\n",
    "                    print(i - 8, item_link)\n",
    "                except Exception as e:\n",
    "                    self.error_logging(e, f\"fail get item links {self.__class__}\")\n",
    "                    break\n",
    "                # if item_link not in self.previous_items_queue:\n",
    "                #     self.item_link_queue.append((item_link, 0))\n",
    "                #     self.previous_items_queue.appendleft(item_link)\n",
    "                #     if len(self.previous_items_queue) > 100:\n",
    "                #         self.previous_items_queue.pop()\n",
    "                # else:\n",
    "                #     pass\n",
    "            time.sleep(30)\n",
    "            get_item_driver.refresh()\n",
    "            \n",
    "    def crawling(self):\n",
    "        driver = self.set_drvier(self.hot_deal_page)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                item_link, retry_attempt = self.item_link_queue.popleft()\n",
    "                print(item_link, retry_attempt)\n",
    "            except:\n",
    "                print(\"Empty Queue\")\n",
    "                break\n",
    "            driver.get(item_link)\n",
    "            time.sleep(5)\n",
    "            try: # 신고 처리, 보안 검사 등\n",
    "                # item_name = driver.find_element(By.CSS_SELECTOR, \"body > div.wrapper > div.contents > div.container > div > table:nth-child(9) > tbody > tr:nth-child(3) > td > table > tbody > tr > td:nth-child(5) > div > div.sub-top-text-box > font.view_title2\").text\n",
    "                # content = driver.find_element(By.CSS_SELECTOR, \"body > div.wrapper > div.contents > div.container > div > table:nth-child(15) > tbody > tr:nth-child(1) > td > table > tbody > tr > td\").text\n",
    "                # comments = driver.find_element(By.ID, \"quote\").text\n",
    "                # shopping_mall_link = driver.find_element(By.CSS_SELECTOR, \"body > div.wrapper > div.contents > div.container > div > table:nth-child(9) > tbody > tr:nth-child(3) > td > table > tbody > tr > td:nth-child(5) > div > div.sub-top-text-box > div > a\").get_attribute(\"href\")\n",
    "                # shopping_mall = driver.find_element(By.CSS_SELECTOR, \"body > div.wrapper > div.contents > div.container > div > table:nth-child(9) > tbody > tr:nth-child(3) > td > table > tbody > tr > td:nth-child(5) > div > div.sub-top-text-box > font.view_title2 > span\").text\n",
    "                item_name = driver.find_element(By.CSS_SELECTOR, \"#topTitle > h1\").text\n",
    "                content = driver.find_element(By.CSS_SELECTOR, \"body > div.wrapper > div.contents > div.container > div > table:nth-child(14) > tbody > tr:nth-child(1) > td > table > tbody > tr > td\").text\n",
    "                comments = driver.find_element(By.ID, \"quote\").text\n",
    "                shopping_mall_link = driver.find_element(By.CSS_SELECTOR, \"#topTitle > div > ul > li.topTitle-link > a\").get_attribute(\"href\")\n",
    "                shopping_mall = driver.find_element(By.CSS_SELECTOR, \"#topTitle > h1 > span.subject_preface.type2\").text\n",
    "                print(item_name, content, comments, shopping_mall, shopping_mall_link)\n",
    "            except Exception as e:\n",
    "                if retry_attempt >= 3:\n",
    "                    self.error_logging(e, f\"fail crawling {self.__class__}\", item_link = item_link)\n",
    "                else:\n",
    "                    self.item_link_queue.append((item_link, retry_attempt + 1))\n",
    "                continue\n",
    "                \n",
    "            self.insert_to_db(item_link = item_link, item_name = item_name, content = content, comments = comments, shopping_mall = shopping_mall, shopping_mall_link = shopping_mall_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quasar_zone = QUASAR_ZONE()\n",
    "ppom_ppu = PPOM_PPU()\n",
    "fm_korea = FM_KOREA()\n",
    "ruli_web = RULI_WEB()\n",
    "arca_live = ARCA_LIVE()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quasar_zone.get_item_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# ThreadPoolExecutor 사용\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    executor.submit(quasar_zone.get_item_links)\n",
    "    # executor.submit(quasar_zone.crawling)\n",
    "    executor.submit(super_crawling)\n",
    "print(\"Both functions have completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
