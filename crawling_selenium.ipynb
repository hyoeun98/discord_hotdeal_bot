{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver import Keys, ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "from collections import deque\n",
    "import json\n",
    "\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "ARCA_LIVE_LINK = \"https://arca.live/b/hotdeal\"\n",
    "RULI_WEB_LINK = \"https://bbs.ruliweb.com/market/board/1020?view=default\"\n",
    "PPOM_PPU_LINK = \"https://www.ppomppu.co.kr/zboard/zboard.php?id=ppomppu\"\n",
    "QUASAR_ZONE_LINK = \"https://quasarzone.com/bbs/qb_saleinfo\"\n",
    "FM_KOREA_LINK = \"https://www.fmkorea.com/hotdeal\"\n",
    "\n",
    "DB_NAME = os.environ.get(\"DB_NAME\")\n",
    "DB_USER = os.environ.get(\"DB_USER\")\n",
    "DB_PASSWORD = os.environ.get(\"DB_PASSWORD\")\n",
    "DB_HOST = os.environ.get(\"DB_HOST\")\n",
    "DB_PORT = os.environ.get(\"DB_PORT\")\n",
    "\n",
    "connection = psycopg2.connect(\n",
    "    dbname = DB_NAME,\n",
    "    user = DB_USER,\n",
    "    password = DB_PASSWORD,\n",
    "    host = DB_HOST,\n",
    "    port = DB_PORT\n",
    ")\n",
    "cursor = connection.cursor()\n",
    "insert_query = sql.SQL(\"\"\"\n",
    "    INSERT INTO pages (site_name_idx, item_link, consume_time)\n",
    "    VALUES (%s, %s, %s)\n",
    "\"\"\")\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    acks=0, # 메시지 전송 완료에 대한 체크\n",
    "    compression_type='gzip', # 메시지 전달할 때 압축(None, gzip, snappy, lz4 등)\n",
    "    bootstrap_servers=['localhost:29092', 'localhost:39092', 'localhost:49092'], # 전달하고자 하는 카프카 브로커의 주소 리스트\n",
    "    value_serializer=lambda x:json.dumps(x, default=str).encode('utf-8'), # 메시지의 값 직렬화\n",
    "    key_serializer=lambda x:json.dumps(x, default=str).encode('utf-8') # 키의 값 직렬화\n",
    ")\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'test', # 토픽명\n",
    "    bootstrap_servers=['localhost:29092', 'localhost:39092', 'localhost:49092'], # 카프카 브로커 주소 리스트\n",
    "    auto_offset_reset='earliest', # 오프셋 위치(earliest:가장 처음, latest: 가장 최근)\n",
    "    enable_auto_commit=True, # 오프셋 자동 커밋 여부\n",
    "    group_id='test-group', # 컨슈머 그룹 식별자\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8')), # 메시지의 값 역직렬화,\n",
    "    key_deserializer=lambda x: json.loads(x.decode('utf-8')), # 키의 값 역직렬화\n",
    "    # consumer_timeout_ms=10000 # 데이터를 기다리는 최대 시간\n",
    ")\n",
    "\n",
    "def set_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "    chrome_options.add_argument('--block-new-web-contents')\n",
    "    chrome_options.add_argument('--window-size=1920x1080')\n",
    "    # chrome_options.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options = chrome_options)\n",
    "    driver.implicitly_wait(5)\n",
    "    return driver\n",
    "    \n",
    "class Crawler:\n",
    "    def __init__(self, consumer):\n",
    "        self.driver = set_driver()\n",
    "        self.consumer = consumer\n",
    "        \n",
    "    def crawling(self, page, item_link, timestamp):\n",
    "        print(item_link)\n",
    "        try:\n",
    "            cursor.execute(insert_query, (page, item_link, timestamp))\n",
    "            connection.commit()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            connection.rollback()\n",
    "            \n",
    "    def consume_pages(self):\n",
    "        for message in self.consumer:\n",
    "            page = message.key\n",
    "            item_link = message.value\n",
    "            timestamp = datetime.fromtimestamp(message.timestamp / 1000)\n",
    "            print(message)\n",
    "            print(page, item_link)\n",
    "            self.crawling(page, item_link, timestamp)\n",
    "    \n",
    "    # def crawling():\n",
    "    #     ...\n",
    "    #     print(\"crawling\")\n",
    "    #     crawling_driver = self.driver\n",
    "    #     while True:\n",
    "    #         try:\n",
    "    #             item_link, retry_attempt = self.item_link_queue.popleft()\n",
    "    #             print(item_link, retry_attempt)\n",
    "    #         except:\n",
    "    #             print(\"Empty Queue\")\n",
    "    #             time.sleep(60)\n",
    "    #             continue\n",
    "    #         crawling_driver.get(item_link)\n",
    "            \n",
    "    #         try: # 신고 처리, 보안 검사 등\n",
    "    #             item_name = crawling_driver.find_element(By.CSS_SELECTOR, \"#content > div > div.sub-content-wrap > div.left-con-wrap > div.common-view-wrap.market-info-view-wrap > div > dl > dt > div:nth-child(1) > h1\").text.split()[2:]\n",
    "    #             item_name = \" \".join(item_name)\n",
    "    #             table = crawling_driver.find_element(By.TAG_NAME, \"table\")\n",
    "    #             rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "    #             content = crawling_driver.find_element(By.CSS_SELECTOR, \"#new_contents\").text\n",
    "    #             comment = list(map(lambda x: x.text, crawling_driver.find_elements(By.CSS_SELECTOR, \"#content > div.sub-content-wrap > div.left-con-wrap > div.reply-wrap > div.reply-area > div.reply-list\")))\n",
    "    #         except Exception as e:\n",
    "    #             if retry_attempt >= 3:\n",
    "    #                 self.error_logging(e, f\"fail crawling {self.__class__}\", item_link = item_link)\n",
    "    #             else:\n",
    "    #                 self.item_link_queue.append((item_link, retry_attempt + 1))\n",
    "    #             continue\n",
    "            \n",
    "    #         details = [row.text for row in rows]\n",
    "    #         shopping_mall_link, shopping_mall, price, delivery, *_ = list(map(lambda x: \"\".join(x.split()[1:]), details))\n",
    "    #         self.insert_to_db(item_link = item_link, shopping_mall_link = shopping_mall_link, shopping_mall = shopping_mall, item_name = item_name, price = price, delivery = delivery, content = content, comment = comment)\n",
    "    \n",
    "class PathFinder:\n",
    "    def __init__(self):\n",
    "        self.driver = set_driver()\n",
    "    \n",
    "class PAGES:\n",
    "    def __init__(self, pathfinder):\n",
    "        self.refresh_delay = 60 # sec\n",
    "        self.pathfinder = pathfinder\n",
    "        \n",
    "    def pub_hot_deal_page(self, item_link): # crawling 할 page를 publish\n",
    "        try:\n",
    "            cursor.execute(\"SELECT EXISTS(SELECT 1 FROM pages WHERE item_link = %s)\", (item_link,))\n",
    "            exists = cursor.fetchone()[0]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            connection.rollback()\n",
    "            \n",
    "        if not exists:\n",
    "            producer.send(topic = 'test', key = self.site_name, value=item_link)\n",
    "            producer.flush()\n",
    "        else:\n",
    "            ...\n",
    "    \n",
    "    def error_logging(self, e: Exception, error_type, **kwargs):\n",
    "        error_log = {\"error_log\": e, \"time\": time.ctime(), \"error_type\": error_type}\n",
    "        if kwargs:\n",
    "            for k, v in kwargs:\n",
    "                error_log[k] = v\n",
    "        # db.error_log.insert_one(error_log)\n",
    "        print(error_log)\n",
    "        \n",
    "class ARCA_LIVE(PAGES): # shopping_mall_link, shopping_mall, item_name, price, delivery, content, comment\n",
    "    def __init__(self, pathfinder):\n",
    "        self.site_name = ARCA_LIVE_LINK\n",
    "        super().__init__(pathfinder)\n",
    "        \n",
    "    def get_item_links(self):\n",
    "        print(\"get_item_links\", self.site_name)\n",
    "        get_item_driver = self.pathfinder.driver\n",
    "        get_item_driver.get(self.site_name)\n",
    "        for i in range(4, 49):\n",
    "            try:\n",
    "                item = get_item_driver.find_element(By.CSS_SELECTOR, f\"body > div.root-container > div.content-wrapper.clearfix > article > div > div.article-list > div.list-table.hybrid > div:nth-child({i}) > div > div > span.vcol.col-title > a\")\n",
    "                item_link = item.get_attribute(\"href\")\n",
    "                self.pub_hot_deal_page(item_link)\n",
    "                print(i, item_link)\n",
    "            except Exception as e:\n",
    "                self.error_logging(e, f\"fail get item links {self.__class__}\")\n",
    "            \n",
    "    def crawling(self):\n",
    "        driver = self.set_drvier()\n",
    "        while True:\n",
    "            try:\n",
    "                item_link, retry_attempt = self.item_link_queue.popleft()\n",
    "                print(item_link, retry_attempt)\n",
    "            except:\n",
    "                print(\"Empty Queue\")\n",
    "                break\n",
    "            driver.get(item_link)\n",
    "            time.sleep(5)\n",
    "            try: # 신고 처리, 보안 검사 등\n",
    "                table = driver.find_element(By.TAG_NAME, \"table\")\n",
    "                rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                details = [row.text for row in rows]\n",
    "                shopping_mall_link, shopping_mall, item_name, price, delivery = list(map(lambda x: \"\".join(x.split()[1:]), details))\n",
    "                content = driver.find_element(By.CSS_SELECTOR, \"body > div.root-container > div.content-wrapper.clearfix > article > div > div.article-wrapper > div.article-body > div.fr-view.article-content\").text\n",
    "                comment_box = driver.find_element(By.CSS_SELECTOR, \"#comment > div.list-area\")\n",
    "                comment = list(map(lambda x: x.text, comment_box.find_elements(By.CLASS_NAME, \"text\")))\n",
    "            except Exception as e:\n",
    "                if retry_attempt >= 3:\n",
    "                    self.error_logging(e, f\"fail crawling {self.__class__}\", item_link = item_link)\n",
    "                else:\n",
    "                    self.item_link_queue.append((item_link, retry_attempt + 1))\n",
    "                continue\n",
    "            \n",
    "            self.insert_to_db(item_link = item_link, shopping_mall_link = shopping_mall_link, shopping_mall = shopping_mall, price = price, item_name = item_name, delivery = delivery, content = content, comment = comment)\n",
    "\n",
    "# shopping_mall_link가 누락된 채로 게시글이 올라옴\n",
    "class RULI_WEB(PAGES): # shopping_mall_link, item_name, content, comment\n",
    "    def __init__(self, pathfinder):\n",
    "        self.site_name = RULI_WEB_LINK\n",
    "        super().__init__(pathfinder)\n",
    "    \n",
    "    def get_item_links(self):\n",
    "        print(\"get_item_links\", self.site_name)\n",
    "        get_item_driver = self.pathfinder.driver\n",
    "        get_item_driver.get(self.site_name)\n",
    "        item_table = get_item_driver.find_elements(By.CSS_SELECTOR, \"#board_list > div > div.board_main.theme_default.theme_white.theme_white > table > tbody > tr\")\n",
    "        for i, item in enumerate(item_table):\n",
    "            try:\n",
    "                if item.get_attribute(\"class\") == \"table_body blocktarget\":\n",
    "                    item_link = item.find_element(By.CSS_SELECTOR, \"td.subject > div > a.deco\").get_attribute(\"href\")\n",
    "                    self.pub_hot_deal_page(item_link)\n",
    "                    print(i, item_link)\n",
    "                else: # 공지, best 핫딜 등\n",
    "                    continue\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.error_logging(e, f\"fail get item links {self.__class__}\")\n",
    "            \n",
    "    def crawling(self):\n",
    "        driver = self.set_drvier()\n",
    "        while True:\n",
    "            try:\n",
    "                item_link, retry_attempt = self.item_link_queue.popleft()\n",
    "                print(item_link, retry_attempt)\n",
    "            except:\n",
    "                print(\"Empty Queue\")\n",
    "                break\n",
    "            driver.get(item_link)\n",
    "            time.sleep(5)\n",
    "            try: # 신고 처리, 보안 검사 등\n",
    "                item_name = driver.find_element(By.CSS_SELECTOR, \"#board_read > div > div.board_main > div.board_main_top > div.user_view > div:nth-child(1) > div > h4 > span > span.subject_inner_text\").text\n",
    "                shopping_mall_link = driver.find_element(By.CSS_SELECTOR, \"#board_read > div > div.board_main > div.board_main_view > div.row.relative > div > div.source_url.box_line_with_shadow > a\").text\n",
    "                content = driver.find_element(By.TAG_NAME, \"article\").text\n",
    "                comment = list(map(lambda x: x.text, driver.find_elements(By.CLASS_NAME, \"comment\")))\n",
    "            except Exception as e:\n",
    "                if retry_attempt >= 3:\n",
    "                    self.error_logging(e, f\"fail crawling {self.__class__}\", item_link = item_link)\n",
    "                else:\n",
    "                    self.item_link_queue.append((item_link, retry_attempt + 1))\n",
    "                continue\n",
    "            \n",
    "            self.insert_to_db(item_link = item_link, shopping_mall_link = shopping_mall_link, item_name = item_name, content = content, comment = comment)\n",
    "        \n",
    "class FM_KOREA(PAGES): # shopping_mall_link, shopping_mall, item_name, price, delivery, content, comment\n",
    "    def __init__(self, pathfinder):\n",
    "        self.site_name = FM_KOREA_LINK\n",
    "        super().__init__(pathfinder)\n",
    "    \n",
    "    def get_item_links(self):\n",
    "        print(\"get_item_links\", self.site_name)\n",
    "        get_item_driver = self.pathfinder.driver\n",
    "        get_item_driver.get(self.site_name)\n",
    "        for i in range(1, 21):\n",
    "            try:\n",
    "                item = get_item_driver.find_element(By.CSS_SELECTOR, f\"#bd_1196365581_0 > div > div.fm_best_widget._bd_pc > ul > li:nth-child({i}) > div > h3 > a\")\n",
    "                item_link = item.get_attribute(\"href\")\n",
    "                self.pub_hot_deal_page(item_link)\n",
    "                print(i, item_link)\n",
    "            except Exception as e:\n",
    "                self.error_logging(e, f\"fail get item links {self.__class__}\")\n",
    "            \n",
    "    def crawling(self):\n",
    "        driver = self.set_drvier()\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                item_link, retry_attempt = self.item_link_queue.popleft()\n",
    "                print(item_link, retry_attempt)\n",
    "            except:\n",
    "                print(\"Empty Queue\")\n",
    "                break\n",
    "            driver.get(item_link)\n",
    "            time.sleep(5)\n",
    "            try: # 신고 처리, 보안 검사 등\n",
    "                details = driver.find_elements(By.CLASS_NAME, \"xe_content\")\n",
    "                shopping_mall_link, shopping_mall, item_name, price, delivery, content, *comment = details\n",
    "                shopping_mall_link, shopping_mall, item_name, price, delivery, content = map(lambda x: x.text, (shopping_mall_link, shopping_mall, item_name, price, delivery, content))\n",
    "                comment = list(map(lambda x: x.text, comment))\n",
    "            except Exception as e:\n",
    "                if retry_attempt >= 3:\n",
    "                    self.error_logging(e, f\"fail crawling {self.__class__}\", item_link = item_link)\n",
    "                else:\n",
    "                    self.item_link_queue.append((item_link, retry_attempt + 1))\n",
    "                continue\n",
    "            \n",
    "            self.insert_to_db(item_link = item_link, shopping_mall_link = shopping_mall_link, shopping_mall = shopping_mall, item_name = item_name, price = price, delivery = delivery, content = content, comment = comment)\n",
    "        \n",
    "class QUASAR_ZONE(PAGES): # shopping_mall_link, shopping_mall, item_name, price, delivery, content, comment\n",
    "    def __init__(self, pathfinder):\n",
    "        self.site_name = QUASAR_ZONE_LINK\n",
    "        super().__init__(pathfinder)\n",
    "        \n",
    "    def get_item_links(self):\n",
    "        print(\"get_item_links\", self.site_name)\n",
    "        get_item_driver = self.pathfinder.driver\n",
    "        get_item_driver.get(self.site_name)\n",
    "        for i in range(1, 31):\n",
    "            try:\n",
    "                item = get_item_driver.find_element(By.CSS_SELECTOR, f\"#frmSearch > div > div.list-board-wrap > div.market-type-list.market-info-type-list.relative > table > tbody > tr:nth-child({i}) > td:nth-child(2) > div > div.market-info-list-cont > p > a\")\n",
    "                item_link = item.get_attribute(\"href\")\n",
    "                self.pub_hot_deal_page(item_link)\n",
    "                print(i, item_link)\n",
    "            except Exception as e:\n",
    "                self.error_logging(e, f\"fail get item links {self.__class__}\")\n",
    "    \n",
    "    def crawling(self):\n",
    "        print(\"crawling\")\n",
    "        crawling_driver = self.pathfinder.driver\n",
    "        while True:\n",
    "            try:\n",
    "                item_link, retry_attempt = self.item_link_queue.popleft()\n",
    "                print(item_link, retry_attempt)\n",
    "            except:\n",
    "                print(\"Empty Queue\")\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "            crawling_driver.get(item_link)\n",
    "            \n",
    "            try: # 신고 처리, 보안 검사 등\n",
    "                item_name = crawling_driver.find_element(By.CSS_SELECTOR, \"#content > div > div.sub-content-wrap > div.left-con-wrap > div.common-view-wrap.market-info-view-wrap > div > dl > dt > div:nth-child(1) > h1\").text.split()[2:]\n",
    "                item_name = \" \".join(item_name)\n",
    "                table = crawling_driver.find_element(By.TAG_NAME, \"table\")\n",
    "                rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                content = crawling_driver.find_element(By.CSS_SELECTOR, \"#new_contents\").text\n",
    "                comment = list(map(lambda x: x.text, crawling_driver.find_elements(By.CSS_SELECTOR, \"#content > div.sub-content-wrap > div.left-con-wrap > div.reply-wrap > div.reply-area > div.reply-list\")))\n",
    "            except Exception as e:\n",
    "                if retry_attempt >= 3:\n",
    "                    self.error_logging(e, f\"fail crawling {self.__class__}\", item_link = item_link)\n",
    "                else:\n",
    "                    self.item_link_queue.append((item_link, retry_attempt + 1))\n",
    "                continue\n",
    "            \n",
    "            details = [row.text for row in rows]\n",
    "            shopping_mall_link, shopping_mall, price, delivery, *_ = list(map(lambda x: \"\".join(x.split()[1:]), details))\n",
    "            self.insert_to_db(item_link = item_link, shopping_mall_link = shopping_mall_link, shopping_mall = shopping_mall, item_name = item_name, price = price, delivery = delivery, content = content, comment = comment)\n",
    "\n",
    "# shopping_mall이 tag되지 않은 채로 올라옴\n",
    "class PPOM_PPU(PAGES):\n",
    "    def __init__(self, pathfinder):\n",
    "        self.site_name = PPOM_PPU_LINK\n",
    "        super().__init__(pathfinder)\n",
    "        \n",
    "    def get_item_links(self):\n",
    "        print(\"get_item_links\", self.site_name)\n",
    "        get_item_driver = self.pathfinder.driver\n",
    "        get_item_driver.get(self.site_name)\n",
    "        for i in range(9, 34):#revolution_main_table > tbody > tr:nth-child(33)\n",
    "            try:#revolution_main_table > tbody > tr:nth-child(9)\n",
    "                item = get_item_driver.find_element(By.CSS_SELECTOR, f\"#revolution_main_table > tbody > tr:nth-child({i}) > td.baseList-space.title > div > div > a\")\n",
    "                item_link = item.get_attribute(\"href\")\n",
    "                self.pub_hot_deal_page(item_link)\n",
    "                print(i - 8, item_link)\n",
    "            except Exception as e:\n",
    "                self.error_logging(e, f\"fail get item links {self.__class__}\")\n",
    "            \n",
    "    def crawling(self):\n",
    "        driver = self.set_drvier()\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                item_link, retry_attempt = self.item_link_queue.popleft()\n",
    "                print(item_link, retry_attempt)\n",
    "            except:\n",
    "                print(\"Empty Queue\")\n",
    "                break\n",
    "            driver.get(item_link)\n",
    "            time.sleep(5)\n",
    "            try: # 신고 처리, 보안 검사 등\n",
    "                # item_name = driver.find_element(By.CSS_SELECTOR, \"body > div.wrapper > div.contents > div.container > div > table:nth-child(9) > tbody > tr:nth-child(3) > td > table > tbody > tr > td:nth-child(5) > div > div.sub-top-text-box > font.view_title2\").text\n",
    "                # content = driver.find_element(By.CSS_SELECTOR, \"body > div.wrapper > div.contents > div.container > div > table:nth-child(15) > tbody > tr:nth-child(1) > td > table > tbody > tr > td\").text\n",
    "                # comments = driver.find_element(By.ID, \"quote\").text\n",
    "                # shopping_mall_link = driver.find_element(By.CSS_SELECTOR, \"body > div.wrapper > div.contents > div.container > div > table:nth-child(9) > tbody > tr:nth-child(3) > td > table > tbody > tr > td:nth-child(5) > div > div.sub-top-text-box > div > a\").get_attribute(\"href\")\n",
    "                # shopping_mall = driver.find_element(By.CSS_SELECTOR, \"body > div.wrapper > div.contents > div.container > div > table:nth-child(9) > tbody > tr:nth-child(3) > td > table > tbody > tr > td:nth-child(5) > div > div.sub-top-text-box > font.view_title2 > span\").text\n",
    "                item_name = driver.find_element(By.CSS_SELECTOR, \"#topTitle > h1\").text\n",
    "                content = driver.find_element(By.CSS_SELECTOR, \"body > div.wrapper > div.contents > div.container > div > table:nth-child(14) > tbody > tr:nth-child(1) > td > table > tbody > tr > td\").text\n",
    "                comments = driver.find_element(By.ID, \"quote\").text\n",
    "                shopping_mall_link = driver.find_element(By.CSS_SELECTOR, \"#topTitle > div > ul > li.topTitle-link > a\").get_attribute(\"href\")\n",
    "                shopping_mall = driver.find_element(By.CSS_SELECTOR, \"#topTitle > h1 > span.subject_preface.type2\").text\n",
    "                print(item_name, content, comments, shopping_mall, shopping_mall_link)\n",
    "            except Exception as e:\n",
    "                if retry_attempt >= 3:\n",
    "                    self.error_logging(e, f\"fail crawling {self.__class__}\", item_link = item_link)\n",
    "                else:\n",
    "                    self.item_link_queue.append((item_link, retry_attempt + 1))\n",
    "                continue\n",
    "                \n",
    "            self.insert_to_db(item_link = item_link, item_name = item_name, content = content, comments = comments, shopping_mall = shopping_mall, shopping_mall_link = shopping_mall_link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler.consume_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathfinder = PathFinder()\n",
    "quasar_zone = QUASAR_ZONE(pathfinder)\n",
    "ppom_ppu = PPOM_PPU(pathfinder)\n",
    "fm_korea = FM_KOREA(pathfinder)\n",
    "ruli_web = RULI_WEB(pathfinder)\n",
    "arca_live = ARCA_LIVE(pathfinder)\n",
    "crawler = Crawler(consumer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 3\n",
    "while cnt:\n",
    "    cnt -= 1\n",
    "    current = time.time()\n",
    "    quasar_zone.get_item_links()\n",
    "    print(time.time() - current)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    current = time.time()\n",
    "    ppom_ppu.get_item_links()\n",
    "    print(time.time() - current)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    current = time.time()\n",
    "    fm_korea.get_item_links()\n",
    "    print(time.time() - current)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    current = time.time()\n",
    "    ruli_web.get_item_links()\n",
    "    print(time.time() - current)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    current = time.time()\n",
    "    arca_live.get_item_links()\n",
    "    print(time.time() - current)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# ThreadPoolExecutor 사용\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    executor.submit(quasar_zone.get_item_links)\n",
    "    executor.submit(arca_live.get_item_links)\n",
    "print(\"Both functions have completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
